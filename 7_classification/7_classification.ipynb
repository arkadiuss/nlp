{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a018c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import pipeline, AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe72e73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset poleval2019_cyber_bullying (/home/arkadius/.cache/huggingface/datasets/poleval2019_cyber_bullying/task01/1.0.0/ce6060c56dae43c469bab309a7573b86299b0bcc2484e85cfe0ae70b5f770450)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272301d6a4d44d91a6247cb560f3e1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset poleval2019_cyber_bullying (/home/arkadius/.cache/huggingface/datasets/poleval2019_cyber_bullying/task02/1.0.0/ce6060c56dae43c469bab309a7573b86299b0bcc2484e85cfe0ae70b5f770450)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7140b0dfeda404e8bd777a28e56fbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset01 = load_dataset(\"poleval2019_cyberbullying\", \"task01\")\n",
    "dataset02 = load_dataset(\"poleval2019_cyberbullying\", \"task02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abd9a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset01 = dataset01['train']\n",
    "test_dataset01 = dataset01['test']\n",
    "train_dataset02 = dataset02['train']\n",
    "test_dataset02 = dataset02['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "697069b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Dla mnie faworytem do tytuu bdzie Cracovia. Zobaczymy, czy typ si sprawdzi.',\n",
       "  '@anonymized_account @anonymized_account Brawo ty Daria kibic ma by na dobre i ze',\n",
       "  '@anonymized_account @anonymized_account Super, polski premier skada kwiaty na grobach kolaborant贸w. Ale doczekalimy czas贸w.',\n",
       "  '@anonymized_account @anonymized_account Musi. Innej drogi nie mamy.',\n",
       "  'Odrzut natychmiastowy, kwana mina, mam problem',\n",
       "  'Jaki on by fajny xdd pamitam, 偶e sp贸藕niam si na jego pierwsze zajcia i to sporo i za kar kaza mi usi w pierwszej awce XD',\n",
       "  '@anonymized_account No nie ma u nas szczcia ',\n",
       "  '@anonymized_account Dawno kogo tak wrednego nie widziaam xd',\n",
       "  '@anonymized_account @anonymized_account Zalegoci byy, ale wa偶ne czy byy wezwania do zapaty z kt贸rych si klub nie wywiza.',\n",
       "  '@anonymized_account @anonymized_account @anonymized_account Gdzie jest @anonymized_account . Brudziski jeste kamc i marnym kutasem @anonymized_account',\n",
       "  '@anonymized_account @anonymized_account  no mam nadzieje !!:)',\n",
       "  '@anonymized_account @anonymized_account Mo偶e gustowa w starszych paniach ;-)',\n",
       "  '@anonymized_account Zostawiam tam 3 lata temu \\\\\"Notatki na mankietach\\\\\". Musz si wr贸ci.',\n",
       "  '@anonymized_account Oprawa do Krzysia M. Ps Pinokio -\\\\n\\\\\" moge by gangsterem a zostae 3 ligowym frajerem \\\\\"',\n",
       "  '@anonymized_account @anonymized_account Znowu bdzie komunikat o polskich piratach drogowych w Sowacji.',\n",
       "  '@anonymized_account M贸wi czowiek, kt贸ry chcia nao偶y sankcje na Polsk. Gratulacje. #Niepodlegla #11lisopada',\n",
       "  'RT @anonymized_account @anonymized_account M贸wi czowiek, kt贸ry chcia nao偶y sankcje na Polsk. Gratulacje. #Niepodlegla #11lisopada',\n",
       "  '@anonymized_account @anonymized_account o jakim zachowaniu \\\\\"fer\\\\\" m贸wisz skoro ukrywa si z tym 偶e podpisa kontrakt z Legi ? Mo偶esz rozwin ?',\n",
       "  '@anonymized_account @anonymized_account Joanno! Po raz pierwszy si z pani zgadzam.',\n",
       "  'Monster block, monster block, monster block... '],\n",
       " 'label': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset01[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "449c69ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Dla mnie faworytem do tytuu bdzie Cracovia. Zobaczymy, czy typ si sprawdzi.',\n",
       "  '@anonymized_account @anonymized_account Brawo ty Daria kibic ma by na dobre i ze',\n",
       "  '@anonymized_account @anonymized_account Super, polski premier skada kwiaty na grobach kolaborant贸w. Ale doczekalimy czas贸w.',\n",
       "  '@anonymized_account @anonymized_account Musi. Innej drogi nie mamy.',\n",
       "  'Odrzut natychmiastowy, kwana mina, mam problem',\n",
       "  'Jaki on by fajny xdd pamitam, 偶e sp贸藕niam si na jego pierwsze zajcia i to sporo i za kar kaza mi usi w pierwszej awce XD',\n",
       "  '@anonymized_account No nie ma u nas szczcia ',\n",
       "  '@anonymized_account Dawno kogo tak wrednego nie widziaam xd',\n",
       "  '@anonymized_account @anonymized_account Zalegoci byy, ale wa偶ne czy byy wezwania do zapaty z kt贸rych si klub nie wywiza.',\n",
       "  '@anonymized_account @anonymized_account @anonymized_account Gdzie jest @anonymized_account . Brudziski jeste kamc i marnym kutasem @anonymized_account',\n",
       "  '@anonymized_account @anonymized_account  no mam nadzieje !!:)',\n",
       "  '@anonymized_account @anonymized_account Mo偶e gustowa w starszych paniach ;-)',\n",
       "  '@anonymized_account Zostawiam tam 3 lata temu \\\\\"Notatki na mankietach\\\\\". Musz si wr贸ci.',\n",
       "  '@anonymized_account Oprawa do Krzysia M. Ps Pinokio -\\\\n\\\\\" moge by gangsterem a zostae 3 ligowym frajerem \\\\\"',\n",
       "  '@anonymized_account @anonymized_account Znowu bdzie komunikat o polskich piratach drogowych w Sowacji.',\n",
       "  '@anonymized_account M贸wi czowiek, kt贸ry chcia nao偶y sankcje na Polsk. Gratulacje. #Niepodlegla #11lisopada',\n",
       "  'RT @anonymized_account @anonymized_account M贸wi czowiek, kt贸ry chcia nao偶y sankcje na Polsk. Gratulacje. #Niepodlegla #11lisopada',\n",
       "  '@anonymized_account @anonymized_account o jakim zachowaniu \\\\\"fer\\\\\" m贸wisz skoro ukrywa si z tym 偶e podpisa kontrakt z Legi ? Mo偶esz rozwin ?',\n",
       "  '@anonymized_account @anonymized_account Joanno! Po raz pierwszy si z pani zgadzam.',\n",
       "  'Monster block, monster block, monster block... '],\n",
       " 'label': [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset02[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cdaa42",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8092f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_columns = [\"Classifier\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "def to_scores_df(model_name,scores_dict):\n",
    "    return pd.DataFrame(data=[[\n",
    "        model_name,\n",
    "        scores_dict[\"accuracy\"],\n",
    "        scores_dict[\"precision\"],\n",
    "        scores_dict[\"recall\"],\n",
    "        scores_dict[\"f1\"],\n",
    "    ]], columns=scores_columns)\n",
    "\n",
    "scores_singleclass_df = pd.DataFrame(data=[], columns=scores_columns)\n",
    "scores_multiclass_df = pd.DataFrame(data=[], columns=scores_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a4ec9",
   "metadata": {},
   "source": [
    "#### Baessian + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb94c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(train_dataset, test_dataset):    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train_dataset)\n",
    "    return vectorizer.transform(train_dataset).toarray(), vectorizer.transform(test_dataset).toarray()\n",
    "\n",
    "train_dataset01_tfidf, test_dataset01_tfidf = tf_idf(train_dataset01['text'], test_dataset01['text'])\n",
    "train_dataset02_tfidf, test_dataset02_tfidf = tf_idf(train_dataset02['text'], test_dataset02['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10330191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb01 = GaussianNB()\n",
    "gnb01.fit(train_dataset01_tfidf, train_dataset01['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3b42673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb02 = GaussianNB()\n",
    "gnb02.fit(train_dataset02_tfidf, train_dataset02['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87a36059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, test_y):\n",
    "    y = predictions\n",
    "    accuracy = accuracy_score(y, test_y)\n",
    "    precision = precision_score(y, test_y)\n",
    "    recall = recall_score(y, test_y)\n",
    "    f1 = f1_score(y, test_y)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} \n",
    "\n",
    "def evaluate_multiclass(predictions, test_y):\n",
    "    y = predictions\n",
    "    accuracy = accuracy_score(y, test_y)\n",
    "    precision = precision_score(y, test_y, average='macro')\n",
    "    recall = recall_score(y, test_y, average='macro')\n",
    "    f1 = f1_score(y, test_y, average='macro')\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "24b260cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb01_scores = evaluate(gnb01.predict(test_dataset01_tfidf), test_dataset01['label'])\n",
    "gnb02_scores = evaluate_multiclass(gnb02.predict(test_dataset02_tfidf), test_dataset02['label'])\n",
    "scores_singleclass_df = scores_singleclass_df.append(to_scores_df(\"GaussianNaiveBayes\", gnb01_scores))\n",
    "scores_multiclass_df = scores_multiclass_df.append(to_scores_df(\"GaussianNaiveBayes\", gnb02_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bec14",
   "metadata": {},
   "source": [
    "#### Fasttext  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c66832c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fasttext.FastText in fasttext:\n",
      "\n",
      "NAME\n",
      "    fasttext.FastText\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the MIT license found in the\n",
      "    # LICENSE file in the root directory of this source tree.\n",
      "\n",
      "FUNCTIONS\n",
      "    cbow(*kargs, **kwargs)\n",
      "    \n",
      "    load_model(path)\n",
      "        Load a model given a filepath and return a model object.\n",
      "    \n",
      "    read_args(arg_list, arg_dict, arg_names, default_values)\n",
      "    \n",
      "    skipgram(*kargs, **kwargs)\n",
      "    \n",
      "    supervised(*kargs, **kwargs)\n",
      "    \n",
      "    tokenize(text)\n",
      "        Given a string of text, tokenize it and return a list of tokens\n",
      "    \n",
      "    train_supervised(*kargs, **kwargs)\n",
      "        Train a supervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input file must must contain at least one label per line. For an\n",
      "        example consult the example datasets which are part of the fastText\n",
      "        repository such as the dataset pulled by classification-example.sh.\n",
      "    \n",
      "    train_unsupervised(*kargs, **kwargs)\n",
      "        Train an unsupervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input field must not contain any labels or use the specified label prefix\n",
      "        unless it is ok for those words to be ignored. For an example consult the\n",
      "        dataset pulled by the example script word-vector-example.sh, which is\n",
      "        part of the fastText repository.\n",
      "\n",
      "DATA\n",
      "    BOW = '<'\n",
      "    EOS = '</s>'\n",
      "    EOW = '>'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    displayed_errors = {}\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "    unsupervised_default = {'autotuneDuration': 300, 'autotuneMetric': 'f1...\n",
      "\n",
      "FILE\n",
      "    /home/arkadius/studies/nlp2/7_classification/venv/lib/python3.6/site-packages/fasttext/FastText.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fasttext.FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d18d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_fast_text_input_file(dataset, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for label, text in zip(dataset['label'], dataset['text']):\n",
    "            f.write(f\"__label__{label} {text}\\n\")\n",
    "\n",
    "to_fast_text_input_file(train_dataset01, 'fasttext_train01.txt')\n",
    "to_fast_text_input_file(test_dataset01, 'fasttext_test01.txt')\n",
    "to_fast_text_input_file(train_dataset02, 'fasttext_train02.txt')\n",
    "to_fast_text_input_file(test_dataset02, 'fasttext_test02.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "14c361ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model01 = fasttext.train_supervised('fasttext_train01.txt')\n",
    "fasttext_model02 = fasttext.train_supervised('fasttext_train02.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0472932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_scores_dict(test_res):\n",
    "    return { \"accuracy\": test_res[1], \"precision\": None, \"recall\": None, \"f1\": None }\n",
    "\n",
    "fasttext_model01_scores = fasttext_scores_dict(fasttext_model01.test('fasttext_test01.txt'))\n",
    "fasttext_model02_scores = fasttext_scores_dict(fasttext_model01.test('fasttext_test02.txt'))\n",
    "scores_singleclass_df = scores_singleclass_df.append(to_scores_df(\"fastText\", fasttext_model01_scores))\n",
    "scores_multiclass_df = scores_multiclass_df.append(to_scores_df(\"fastText\", fasttext_model02_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158a846",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "https://huggingface.co/docs/transformers/custom_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6c7d900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized(tokenizer_name, dataset):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "    tokenized_dt = dataset.map(preprocess_function, batched=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    return tokenizer, tokenized_dt\n",
    "\n",
    "def fine_tuned(model_name, dataset, expected_labels):\n",
    "    print(\"Tokenization\")\n",
    "    tokenizer, tokenized_dt = tokenized(model_name, dataset)\n",
    "    print(\"Training\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=expected_labels)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dt[\"train\"],\n",
    "        eval_dataset=tokenized_dt[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a188a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# herbert_fine_tuned = fine_tuned(\"allegro/herbert-base-cased\", dataset01, 2)\n",
    "# herbert_fine_tuned.save_pretrained(\"herbert-base-cased-bullying\")\n",
    "\n",
    "herbert_fine_tuned = AutoModelForSequenceClassification.from_pretrained(\"herbert-base-cased-bullying\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20ec4ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# herbert_fine_tuned02 = fine_tuned(\"allegro/herbert-base-cased\", dataset02, 3)\n",
    "# herbert_fine_tuned02.save_pretrained(\"herbert-base-cased-bullying02\")\n",
    "\n",
    "herbert_fine_tuned02 =  AutoModelForSequenceClassification.from_pretrained(\"herbert-base-cased-bullying02\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "88d1e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_singleclass(p):    \n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    return evaluate(pred, labels)\n",
    "\n",
    "def compute_metrics_multiclass(p):    \n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    return evaluate_multiclass(pred, labels)\n",
    "\n",
    "def evaluate_transformers(model, dataset, tokenizer_name, compute_metrics):\n",
    "    print(\"Tokenization\")\n",
    "    tokenizer, tokenized_dt = tokenized(tokenizer_name, dataset)\n",
    "    print(\"Evaluation\")\n",
    "    trainer = Trainer(model=model,\n",
    "                      eval_dataset=tokenized_dt[\"test\"],\n",
    "                      tokenizer=tokenizer,\n",
    "                      compute_metrics=compute_metrics)\n",
    "\n",
    "    return trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "090dbf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/allegro/herbert-base-cased/resolve/main/config.json from cache at /home/arkadius/.cache/huggingface/transformers/d24c58747dbe6b61ed3e1eb5d488dfec9332ed13dd3f8983588f30d96f6f1bde.193ae07fbea6bb9ac46f854cd03094e486dfa4483e0596fd6a159dcfaef521a5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/vocab.json from cache at /home/arkadius/.cache/huggingface/transformers/af5d37bc47d0255f7b43bac58d73fc8c0a28c4f068660395c79eb2b2727c2195.adf299ef80f3ee4dff5abe4e65a8ff8b436992d5df16b7ed0a192de08e0b142d\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/merges.txt from cache at /home/arkadius/.cache/huggingface/transformers/92f73d3fb4e52e4102dfc8ee8c32e4f7e4876230a9ff5d01a204a1f688376e08.b43456cc5f4fc752a150d7e61b1fe9eec2c958f067aa08884125223b8872d968\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/special_tokens_map.json from cache at /home/arkadius/.cache/huggingface/transformers/a77c29d6c21e653166565b0d51ea0a46b918191512e0ae894422060267a0f436.b8e113717eb1828d09e47de853cf49c8fad05ebdce24df2614cd942dc23e2a77\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/tokenizer_config.json from cache at /home/arkadius/.cache/huggingface/transformers/12b5777c81f4e1e28754b35fc5d5323167b9498c934ca9ab659387b97f3f4627.5399d59cc1c4147c064f53c77a985e2d758532b64d5d5a3adc8653637876150d\n",
      "loading configuration file https://huggingface.co/allegro/herbert-base-cased/resolve/main/config.json from cache at /home/arkadius/.cache/huggingface/transformers/d24c58747dbe6b61ed3e1eb5d488dfec9332ed13dd3f8983588f30d96f6f1bde.193ae07fbea6bb9ac46f854cd03094e486dfa4483e0596fd6a159dcfaef521a5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/allegro/herbert-base-cased/resolve/main/config.json from cache at /home/arkadius/.cache/huggingface/transformers/d24c58747dbe6b61ed3e1eb5d488dfec9332ed13dd3f8983588f30d96f6f1bde.193ae07fbea6bb9ac46f854cd03094e486dfa4483e0596fd6a159dcfaef521a5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/arkadius/.cache/huggingface/datasets/poleval2019_cyber_bullying/task01/1.0.0/ce6060c56dae43c469bab309a7573b86299b0bcc2484e85cfe0ae70b5f770450/cache-792f34e4b3f51eff.arrow\n",
      "Loading cached processed dataset at /home/arkadius/.cache/huggingface/datasets/poleval2019_cyber_bullying/task01/1.0.0/ce6060c56dae43c469bab309a7573b86299b0bcc2484e85cfe0ae70b5f770450/cache-4b44edfdcf0aa81c.arrow\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/allegro/herbert-base-cased/resolve/main/config.json from cache at /home/arkadius/.cache/huggingface/transformers/d24c58747dbe6b61ed3e1eb5d488dfec9332ed13dd3f8983588f30d96f6f1bde.193ae07fbea6bb9ac46f854cd03094e486dfa4483e0596fd6a159dcfaef521a5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/vocab.json from cache at /home/arkadius/.cache/huggingface/transformers/af5d37bc47d0255f7b43bac58d73fc8c0a28c4f068660395c79eb2b2727c2195.adf299ef80f3ee4dff5abe4e65a8ff8b436992d5df16b7ed0a192de08e0b142d\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/merges.txt from cache at /home/arkadius/.cache/huggingface/transformers/92f73d3fb4e52e4102dfc8ee8c32e4f7e4876230a9ff5d01a204a1f688376e08.b43456cc5f4fc752a150d7e61b1fe9eec2c958f067aa08884125223b8872d968\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/special_tokens_map.json from cache at /home/arkadius/.cache/huggingface/transformers/a77c29d6c21e653166565b0d51ea0a46b918191512e0ae894422060267a0f436.b8e113717eb1828d09e47de853cf49c8fad05ebdce24df2614cd942dc23e2a77\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/tokenizer_config.json from cache at /home/arkadius/.cache/huggingface/transformers/12b5777c81f4e1e28754b35fc5d5323167b9498c934ca9ab659387b97f3f4627.5399d59cc1c4147c064f53c77a985e2d758532b64d5d5a3adc8653637876150d\n",
      "loading configuration file https://huggingface.co/allegro/herbert-base-cased/resolve/main/config.json from cache at /home/arkadius/.cache/huggingface/transformers/d24c58747dbe6b61ed3e1eb5d488dfec9332ed13dd3f8983588f30d96f6f1bde.193ae07fbea6bb9ac46f854cd03094e486dfa4483e0596fd6a159dcfaef521a5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/allegro/herbert-base-cased/resolve/main/config.json from cache at /home/arkadius/.cache/huggingface/transformers/d24c58747dbe6b61ed3e1eb5d488dfec9332ed13dd3f8983588f30d96f6f1bde.193ae07fbea6bb9ac46f854cd03094e486dfa4483e0596fd6a159dcfaef521a5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/arkadius/.cache/huggingface/datasets/poleval2019_cyber_bullying/task02/1.0.0/ce6060c56dae43c469bab309a7573b86299b0bcc2484e85cfe0ae70b5f770450/cache-6a374f94e8419417.arrow\n",
      "Loading cached processed dataset at /home/arkadius/.cache/huggingface/datasets/poleval2019_cyber_bullying/task02/1.0.0/ce6060c56dae43c469bab309a7573b86299b0bcc2484e85cfe0ae70b5f770450/cache-7b3e62fd827edb8b.arrow\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transformers_to_scores(eval_res):\n",
    "    return {\"accuracy\": eval_res[\"eval_accuracy\"], \"precision\": eval_res[\"eval_precision\"], \"recall\": eval_res[\"eval_recall\"], \"f1\": eval_res[\"eval_f1\"]} \n",
    "\n",
    "herbert_fine_tuned_score = transformers_to_scores(\n",
    "    evaluate_transformers(herbert_fine_tuned, dataset01, \"allegro/herbert-base-cased\", compute_metrics_multiclass))\n",
    "herbert_fine_tuned02_score = transformers_to_scores(\n",
    "    evaluate_transformers(herbert_fine_tuned02, dataset02, \"allegro/herbert-base-cased\", compute_metrics_multiclass))\n",
    "\n",
    "scores_singleclass_df = scores_singleclass_df.append(to_scores_df(\"transformers_herbert-cased\", herbert_fine_tuned_score))\n",
    "scores_multiclass_df = scores_multiclass_df.append(to_scores_df(\"transformers_herbert-cased\", herbert_fine_tuned02_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe1ef7",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "964c5920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNaiveBayes</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.268456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fastText</td>\n",
       "      <td>0.873</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformers_herbert_cased</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.701717</td>\n",
       "      <td>0.744613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformers_herbert_cased</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.701717</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.744613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformers_herbert-cased</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.701717</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.744613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Classifier  Accuracy Precision    Recall        F1\n",
       "0          GaussianNaiveBayes     0.782  0.298507  0.243902  0.268456\n",
       "0                    fastText     0.873      None      None      None\n",
       "0  transformers_herbert_cased     0.904  0.833333  0.701717  0.744613\n",
       "0  transformers_herbert_cased     0.904  0.701717  0.833333  0.744613\n",
       "0  transformers_herbert-cased     0.904  0.701717  0.833333  0.744613"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_singleclass_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5cce713b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNaiveBayes</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.408183</td>\n",
       "      <td>0.401325</td>\n",
       "      <td>0.396831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fastText</td>\n",
       "      <td>0.968575</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformers_herbert_cased</td>\n",
       "      <td>0.881000</td>\n",
       "      <td>0.5762</td>\n",
       "      <td>0.463077</td>\n",
       "      <td>0.488148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformers_herbert_cased</td>\n",
       "      <td>0.881000</td>\n",
       "      <td>0.463077</td>\n",
       "      <td>0.5762</td>\n",
       "      <td>0.488148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformers_herbert-cased</td>\n",
       "      <td>0.881000</td>\n",
       "      <td>0.463077</td>\n",
       "      <td>0.5762</td>\n",
       "      <td>0.488148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Classifier  Accuracy Precision    Recall        F1\n",
       "0          GaussianNaiveBayes  0.787000  0.408183  0.401325  0.396831\n",
       "0                    fastText  0.968575      None      None      None\n",
       "0  transformers_herbert_cased  0.881000    0.5762  0.463077  0.488148\n",
       "0  transformers_herbert_cased  0.881000  0.463077    0.5762  0.488148\n",
       "0  transformers_herbert-cased  0.881000  0.463077    0.5762  0.488148"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_multiclass_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3edf68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02816c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
