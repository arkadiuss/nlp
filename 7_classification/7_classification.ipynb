{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4a018c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import pipeline, AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe72e73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset poleval2019_cyber_bullying (/home/arkadius/.cache/huggingface/datasets/poleval2019_cyber_bullying/task01/1.0.0/ce6060c56dae43c469bab309a7573b86299b0bcc2484e85cfe0ae70b5f770450)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5d988065344eac9528e382d889a31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset poleval2019_cyber_bullying (/home/arkadius/.cache/huggingface/datasets/poleval2019_cyber_bullying/task02/1.0.0/ce6060c56dae43c469bab309a7573b86299b0bcc2484e85cfe0ae70b5f770450)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9cebc0c8d34784a95fb10f387a14ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset01 = load_dataset(\"poleval2019_cyberbullying\", \"task01\")\n",
    "dataset02 = load_dataset(\"poleval2019_cyberbullying\", \"task02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abd9a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset01 = dataset01['train']\n",
    "test_dataset01 = dataset01['test']\n",
    "train_dataset02 = dataset02['train']\n",
    "test_dataset02 = dataset02['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "019a1b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Dla mnie faworytem do tytuu bdzie Cracovia. Zobaczymy, czy typ si sprawdzi.',\n",
       "  '@anonymized_account @anonymized_account Brawo ty Daria kibic ma by na dobre i ze',\n",
       "  '@anonymized_account @anonymized_account Super, polski premier skada kwiaty na grobach kolaborant贸w. Ale doczekalimy czas贸w.',\n",
       "  '@anonymized_account @anonymized_account Musi. Innej drogi nie mamy.',\n",
       "  'Odrzut natychmiastowy, kwana mina, mam problem',\n",
       "  'Jaki on by fajny xdd pamitam, 偶e sp贸藕niam si na jego pierwsze zajcia i to sporo i za kar kaza mi usi w pierwszej awce XD',\n",
       "  '@anonymized_account No nie ma u nas szczcia ',\n",
       "  '@anonymized_account Dawno kogo tak wrednego nie widziaam xd',\n",
       "  '@anonymized_account @anonymized_account Zalegoci byy, ale wa偶ne czy byy wezwania do zapaty z kt贸rych si klub nie wywiza.',\n",
       "  '@anonymized_account @anonymized_account @anonymized_account Gdzie jest @anonymized_account . Brudziski jeste kamc i marnym kutasem @anonymized_account',\n",
       "  '@anonymized_account @anonymized_account  no mam nadzieje !!:)',\n",
       "  '@anonymized_account @anonymized_account Mo偶e gustowa w starszych paniach ;-)',\n",
       "  '@anonymized_account Zostawiam tam 3 lata temu \\\\\"Notatki na mankietach\\\\\". Musz si wr贸ci.',\n",
       "  '@anonymized_account Oprawa do Krzysia M. Ps Pinokio -\\\\n\\\\\" moge by gangsterem a zostae 3 ligowym frajerem \\\\\"',\n",
       "  '@anonymized_account @anonymized_account Znowu bdzie komunikat o polskich piratach drogowych w Sowacji.',\n",
       "  '@anonymized_account M贸wi czowiek, kt贸ry chcia nao偶y sankcje na Polsk. Gratulacje. #Niepodlegla #11lisopada',\n",
       "  'RT @anonymized_account @anonymized_account M贸wi czowiek, kt贸ry chcia nao偶y sankcje na Polsk. Gratulacje. #Niepodlegla #11lisopada',\n",
       "  '@anonymized_account @anonymized_account o jakim zachowaniu \\\\\"fer\\\\\" m贸wisz skoro ukrywa si z tym 偶e podpisa kontrakt z Legi ? Mo偶esz rozwin ?',\n",
       "  '@anonymized_account @anonymized_account Joanno! Po raz pierwszy si z pani zgadzam.',\n",
       "  'Monster block, monster block, monster block... '],\n",
       " 'label': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset01[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e986387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Dla mnie faworytem do tytuu bdzie Cracovia. Zobaczymy, czy typ si sprawdzi.',\n",
       "  '@anonymized_account @anonymized_account Brawo ty Daria kibic ma by na dobre i ze',\n",
       "  '@anonymized_account @anonymized_account Super, polski premier skada kwiaty na grobach kolaborant贸w. Ale doczekalimy czas贸w.',\n",
       "  '@anonymized_account @anonymized_account Musi. Innej drogi nie mamy.',\n",
       "  'Odrzut natychmiastowy, kwana mina, mam problem',\n",
       "  'Jaki on by fajny xdd pamitam, 偶e sp贸藕niam si na jego pierwsze zajcia i to sporo i za kar kaza mi usi w pierwszej awce XD',\n",
       "  '@anonymized_account No nie ma u nas szczcia ',\n",
       "  '@anonymized_account Dawno kogo tak wrednego nie widziaam xd',\n",
       "  '@anonymized_account @anonymized_account Zalegoci byy, ale wa偶ne czy byy wezwania do zapaty z kt贸rych si klub nie wywiza.',\n",
       "  '@anonymized_account @anonymized_account @anonymized_account Gdzie jest @anonymized_account . Brudziski jeste kamc i marnym kutasem @anonymized_account',\n",
       "  '@anonymized_account @anonymized_account  no mam nadzieje !!:)',\n",
       "  '@anonymized_account @anonymized_account Mo偶e gustowa w starszych paniach ;-)',\n",
       "  '@anonymized_account Zostawiam tam 3 lata temu \\\\\"Notatki na mankietach\\\\\". Musz si wr贸ci.',\n",
       "  '@anonymized_account Oprawa do Krzysia M. Ps Pinokio -\\\\n\\\\\" moge by gangsterem a zostae 3 ligowym frajerem \\\\\"',\n",
       "  '@anonymized_account @anonymized_account Znowu bdzie komunikat o polskich piratach drogowych w Sowacji.',\n",
       "  '@anonymized_account M贸wi czowiek, kt贸ry chcia nao偶y sankcje na Polsk. Gratulacje. #Niepodlegla #11lisopada',\n",
       "  'RT @anonymized_account @anonymized_account M贸wi czowiek, kt贸ry chcia nao偶y sankcje na Polsk. Gratulacje. #Niepodlegla #11lisopada',\n",
       "  '@anonymized_account @anonymized_account o jakim zachowaniu \\\\\"fer\\\\\" m贸wisz skoro ukrywa si z tym 偶e podpisa kontrakt z Legi ? Mo偶esz rozwin ?',\n",
       "  '@anonymized_account @anonymized_account Joanno! Po raz pierwszy si z pani zgadzam.',\n",
       "  'Monster block, monster block, monster block... '],\n",
       " 'label': [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset02[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0958cd5",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7293ec",
   "metadata": {},
   "source": [
    "#### Baessian + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4a857af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(train_dataset, test_dataset):    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train_dataset)\n",
    "    return vectorizer.transform(train_dataset).toarray(), vectorizer.transform(test_dataset).toarray()\n",
    "\n",
    "train_dataset01_tfidf, test_dataset01_tfidf = tf_idf(train_dataset01['text'], test_dataset01['text'])\n",
    "train_dataset02_tfidf, test_dataset02_tfidf = tf_idf(train_dataset02['text'], test_dataset02['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1340f29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb01 = GaussianNB()\n",
    "gnb01.fit(train_dataset01_tfidf, train_dataset01['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "978609d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.782"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(gnb01.predict(test_dataset01_tfidf), test_dataset01['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3a2fd87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb02 = GaussianNB()\n",
    "gnb02.fit(train_dataset02_tfidf, train_dataset02['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f85a3dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.787"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(gnb02.predict(test_dataset02_tfidf), test_dataset02['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788bab3",
   "metadata": {},
   "source": [
    "#### Fasttext  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d53be26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fasttext.FastText in fasttext:\n",
      "\n",
      "NAME\n",
      "    fasttext.FastText\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the MIT license found in the\n",
      "    # LICENSE file in the root directory of this source tree.\n",
      "\n",
      "FUNCTIONS\n",
      "    cbow(*kargs, **kwargs)\n",
      "    \n",
      "    load_model(path)\n",
      "        Load a model given a filepath and return a model object.\n",
      "    \n",
      "    read_args(arg_list, arg_dict, arg_names, default_values)\n",
      "    \n",
      "    skipgram(*kargs, **kwargs)\n",
      "    \n",
      "    supervised(*kargs, **kwargs)\n",
      "    \n",
      "    tokenize(text)\n",
      "        Given a string of text, tokenize it and return a list of tokens\n",
      "    \n",
      "    train_supervised(*kargs, **kwargs)\n",
      "        Train a supervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input file must must contain at least one label per line. For an\n",
      "        example consult the example datasets which are part of the fastText\n",
      "        repository such as the dataset pulled by classification-example.sh.\n",
      "    \n",
      "    train_unsupervised(*kargs, **kwargs)\n",
      "        Train an unsupervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input field must not contain any labels or use the specified label prefix\n",
      "        unless it is ok for those words to be ignored. For an example consult the\n",
      "        dataset pulled by the example script word-vector-example.sh, which is\n",
      "        part of the fastText repository.\n",
      "\n",
      "DATA\n",
      "    BOW = '<'\n",
      "    EOS = '</s>'\n",
      "    EOW = '>'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    displayed_errors = {}\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "    unsupervised_default = {'autotuneDuration': 300, 'autotuneMetric': 'f1...\n",
      "\n",
      "FILE\n",
      "    /home/arkadius/studies/nlp2/7_classification/venv/lib/python3.6/site-packages/fasttext/FastText.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fasttext.FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1c077d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_fast_text_input_file(dataset, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for label, text in zip(dataset['label'], dataset['text']):\n",
    "            f.write(f\"__label__{label} {text}\\n\")\n",
    "\n",
    "to_fast_text_input_file(train_dataset01, 'fasttext_train01.txt')\n",
    "to_fast_text_input_file(test_dataset01, 'fasttext_test01.txt')\n",
    "to_fast_text_input_file(train_dataset02, 'fasttext_train02.txt')\n",
    "to_fast_text_input_file(test_dataset02, 'fasttext_test02.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df4af2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model01 = fasttext.train_supervised('fasttext_train01.txt')\n",
    "fasttext_model02 = fasttext.train_supervised('fasttext_train02.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "731a3ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 0.872, 0.872)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model01.test('fasttext_test01.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f60ff5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 0.867, 0.867)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model02.test('fasttext_test02.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e563488",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "https://huggingface.co/docs/transformers/custom_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "12f6a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuned(model_name, dataset, expected_labels):\n",
    "    print(\"Tokenization\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "    tokenized_dt = dataset.map(preprocess_function, batched=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=expected_labels)\n",
    "    print(\"Training\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        label_names=[\"label\"]\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dt[\"train\"],\n",
    "        eval_dataset=tokenized_dt[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3801e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/allegro/herbert-base-cased/resolve/main/config.json from cache at /home/arkadius/.cache/huggingface/transformers/d24c58747dbe6b61ed3e1eb5d488dfec9332ed13dd3f8983588f30d96f6f1bde.193ae07fbea6bb9ac46f854cd03094e486dfa4483e0596fd6a159dcfaef521a5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/vocab.json from cache at /home/arkadius/.cache/huggingface/transformers/af5d37bc47d0255f7b43bac58d73fc8c0a28c4f068660395c79eb2b2727c2195.adf299ef80f3ee4dff5abe4e65a8ff8b436992d5df16b7ed0a192de08e0b142d\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/merges.txt from cache at /home/arkadius/.cache/huggingface/transformers/92f73d3fb4e52e4102dfc8ee8c32e4f7e4876230a9ff5d01a204a1f688376e08.b43456cc5f4fc752a150d7e61b1fe9eec2c958f067aa08884125223b8872d968\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/special_tokens_map.json from cache at /home/arkadius/.cache/huggingface/transformers/a77c29d6c21e653166565b0d51ea0a46b918191512e0ae894422060267a0f436.b8e113717eb1828d09e47de853cf49c8fad05ebdce24df2614cd942dc23e2a77\n",
      "loading file https://huggingface.co/allegro/herbert-base-cased/resolve/main/tokenizer_config.json from cache at /home/arkadius/.cache/huggingface/transformers/12b5777c81f4e1e28754b35fc5d5323167b9498c934ca9ab659387b97f3f4627.5399d59cc1c4147c064f53c77a985e2d758532b64d5d5a3adc8653637876150d\n",
      "loading configuration file https://huggingface.co/allegro/herbert-base-cased/resolve/main/config.json from cache at /home/arkadius/.cache/huggingface/transformers/d24c58747dbe6b61ed3e1eb5d488dfec9332ed13dd3f8983588f30d96f6f1bde.193ae07fbea6bb9ac46f854cd03094e486dfa4483e0596fd6a159dcfaef521a5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/allegro/herbert-base-cased/resolve/main/config.json from cache at /home/arkadius/.cache/huggingface/transformers/d24c58747dbe6b61ed3e1eb5d488dfec9332ed13dd3f8983588f30d96f6f1bde.193ae07fbea6bb9ac46f854cd03094e486dfa4483e0596fd6a159dcfaef521a5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/arkadius/.cache/huggingface/datasets/poleval2019_cyber_bullying/task01/1.0.0/ce6060c56dae43c469bab309a7573b86299b0bcc2484e85cfe0ae70b5f770450/cache-792f34e4b3f51eff.arrow\n",
      "Loading cached processed dataset at /home/arkadius/.cache/huggingface/datasets/poleval2019_cyber_bullying/task01/1.0.0/ce6060c56dae43c469bab309a7573b86299b0bcc2484e85cfe0ae70b5f770450/cache-4b44edfdcf0aa81c.arrow\n",
      "loading configuration file https://huggingface.co/allegro/herbert-base-cased/resolve/main/config.json from cache at /home/arkadius/.cache/huggingface/transformers/d24c58747dbe6b61ed3e1eb5d488dfec9332ed13dd3f8983588f30d96f6f1bde.193ae07fbea6bb9ac46f854cd03094e486dfa4483e0596fd6a159dcfaef521a5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/allegro/herbert-base-cased/resolve/main/pytorch_model.bin from cache at /home/arkadius/.cache/huggingface/transformers/8d94420e1bdde1a7acae82f80c251c28c49894c514e11bdca995a9b470b6faec.d0145926eb11b86666593f33913c5d4c4bf925c204f34304e0cdd6e00a39a214\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.sso.sso_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.sso.sso_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 10041\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    }
   ],
   "source": [
    "herbert_fine_tuned = fine_tuned(\"allegro/herbert-base-cased\", dataset01, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a189a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
